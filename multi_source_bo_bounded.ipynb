{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826ca0dcff42a3ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-Information Source BO with Augmented Gaussian Processes\n",
    "- Contributors: andreaponti5\n",
    "- Last updated: Jan 29, 2024\n",
    "- BoTorch version: 0.9.5(dev)\n",
    "\n",
    "In this tutorial, we show how to perform Multiple Information Source Bayesian Optimization in BoTorch based on the Augmented Gaussian Process (AGP) and the Augmented UCB (AUCB) acquisition function proposed in [1].\n",
    "The key idea of the AGP is to fit a GP model for each information source and *augment* the observations on the high fidelity source with those from *cheaper* sources which can be considered as *reliable*. The GP model fitted on this *augmented* set of observations is the AGP.\n",
    "The AUCB is a modification of the standard UCB -- computed on the AGP -- suitably proposed to also deal with the source-specific query cost.\n",
    "\n",
    "We emprically show that the *AGP-based* Multiple Information Source Basyesian Optimization usually performs better than other multi-fidelity approaches [2].\n",
    "\n",
    "[1] [Candelieri, A., & Archetti, F. (2021). Sparsifying to optimize over multiple information sources: an augmented Gaussian process based algorithm. Structural and Multidisciplinary Optimization, 64, 239-255.](https://link.springer.com/article/10.1007/s00158-021-02882-7)\n",
    "[2] [The arxiv will be available soon.](https://arxiv.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa9032dbb2c2b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:54:55.714265100Z",
     "start_time": "2024-02-08T07:54:52.594530700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.49.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m766.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting numpy<2,>=1.21 (from matplotlib)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/vscode/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (303 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.9/303.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.49.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 kiwisolver-1.4.5 matplotlib-3.8.3 numpy-1.26.4 pillow-10.2.0 pyparsing-3.1.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:04\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2023.10.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.1 typing-extensions-4.10.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vscode/.local/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.21.3 tokenizers-0.15.2 transformers-4.38.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (0.21.3)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vscode/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.17.1-py3-none-any.whl (536 kB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.3 datasets-2.17.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting botorch\n",
      "  Using cached botorch-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: multipledispatch in /home/vscode/.local/lib/python3.12/site-packages (from botorch) (1.0.0)\n",
      "Requirement already satisfied: scipy in /home/vscode/.local/lib/python3.12/site-packages (from botorch) (1.12.0)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/vscode/.local/lib/python3.12/site-packages (from botorch) (1.3.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /home/vscode/.local/lib/python3.12/site-packages (from botorch) (2.2.1)\n",
      "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
      "  Using cached pyro_ppl-1.9.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting gpytorch==1.11 (from botorch)\n",
      "  Using cached gpytorch-1.11-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting linear-operator==0.5.1 (from botorch)\n",
      "  Using cached linear_operator-0.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/vscode/.local/lib/python3.12/site-packages (from gpytorch==1.11->botorch) (1.4.1.post1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.9 in /home/vscode/.local/lib/python3.12/site-packages (from linear-operator==0.5.1->botorch) (0.2.25)\n",
      "Requirement already satisfied: typeguard~=2.13.3 in /home/vscode/.local/lib/python3.12/site-packages (from linear-operator==0.5.1->botorch) (2.13.3)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/vscode/.local/lib/python3.12/site-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/vscode/.local/lib/python3.12/site-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.36 in /home/vscode/.local/lib/python3.12/site-packages (from pyro-ppl>=1.8.4->botorch) (4.66.2)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vscode/.local/lib/python3.12/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vscode/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->botorch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vscode/.local/lib/python3.12/site-packages (from jinja2->torch>=1.13.1->botorch) (2.1.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn->gpytorch==1.11->botorch) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn->gpytorch==1.11->botorch) (3.3.0)\n",
      "Using cached botorch-0.10.0-py3-none-any.whl (613 kB)\n",
      "Using cached gpytorch-1.11-py3-none-any.whl (266 kB)\n",
      "Using cached linear_operator-0.5.1-py3-none-any.whl (174 kB)\n",
      "Using cached pyro_ppl-1.9.0-py3-none-any.whl (745 kB)\n",
      "Installing collected packages: pyro-ppl, linear-operator, gpytorch, botorch\n",
      "Successfully installed botorch-0.10.0 gpytorch-1.11 linear-operator-0.5.1 pyro-ppl-1.9.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "required_packages = ['matplotlib', 'numpy', 'torch', 'transformers', 'datasets', 'botorch', 'gpytorch', 'scipy']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55defd1ee4a5b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:06.365087700Z",
     "start_time": "2024-02-08T07:54:55.714265100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "\n",
    "import botorch\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.acquisition import InverseCostWeightedUtility, qMultiFidelityMaxValueEntropy\n",
    "from botorch_community.acquisition.augmented_multisource import AugmentedUpperConfidenceBound\n",
    "from botorch.models import AffineFidelityCostModel, SingleTaskMultiFidelityGP\n",
    "from botorch_community.models.gp_regression_multisource import SingleTaskAugmentedGP, get_random_x_for_agp\n",
    "from botorch.models.transforms import Standardize\n",
    "from botorch.optim import optimize_acqf, optimize_acqf_mixed\n",
    "from botorch.test_functions.multi_fidelity import AugmentedBranin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:07.689089500Z",
     "start_time": "2024-02-08T07:55:06.361808100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e316bd291459a135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:07.701088200Z",
     "start_time": "2024-02-08T07:55:07.690705Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ITER = 10 if SMOKE_TEST else 50\n",
    "SEED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58c67f5dbf329c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem setup\n",
    "We consider the augmented Branin multi-fidelity synthetic test problem. It is important to clarify that *augmented* is not about the AGP: here, it has a different meaning. It means that the Branin test function has been modified by introducing an additional dimension representing the fidelity parameter.\n",
    "\n",
    "The test function takes the form $f(x,s)$ where $x \\in [-5, 10] \\times [0, 15]$ and $s \\in [0,1]$. The target fidelity is 1.0, which means that our goal is to solve $\\max_x f(x,1.0)$ by making use of cheaper evaluations $f(x,s)$ for $s < 1.0$. In this example, we'll assume that the cost function takes the form $5.0 + s$, illustrating a situation where the fixed cost is $5.0$.\n",
    "\n",
    "Since a multiple information source context is considered, three different sources are considered, with $s = 0.5, 0.75, 1.00$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a382cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Bound = tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1001e+04, 1.1001e+04, 1.1001e+04, 1.1001e+04, 1.1001e+04, 1.1001e+04,\n",
      "         1.1001e+04, 1.1001e+04, 1.1001e+04, 1.1001e+04, 2.0000e+00]],\n",
      "       dtype=torch.float64);\n"
     ]
    }
   ],
   "source": [
    "problem = AugmentedBranin(negate=True).to(**tkwargs)\n",
    "fidelities = torch.tensor([0.5, 0.75, 1.0], **tkwargs)\n",
    "n_sources = fidelities.shape[0]\n",
    "\n",
    "# dim = self.dim\n",
    "# max_idx =self.max_idx\n",
    "n_sources = 3\n",
    "dim = 10\n",
    "max_idx = 11001\n",
    "bounds = torch.tensor([[0] * (dim + 1), [max_idx] * (dim + 1)], dtype=torch.float64)\n",
    "bounds[-1,-1] = (n_sources - 1)\n",
    "print(f\"\\t Bound = {bounds};\")\n",
    "\n",
    "problem = AugmentedBranin(negate=True).to(**tkwargs)\n",
    "target_fidelities = {n_sources - 1: 1.0}\n",
    "cost_model = AffineFidelityCostModel(fidelity_weights=target_fidelities, fixed_cost=1.1)\n",
    "cost_aware_utility = InverseCostWeightedUtility(cost_model=cost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f13380e681011ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:07.793085900Z",
     "start_time": "2024-02-08T07:55:07.698958300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# problem = AugmentedBranin(negate=True).to(**tkwargs)\n",
    "# fidelities = torch.tensor([0.5, 0.75, 1.0], **tkwargs)\n",
    "# n_sources = fidelities.shape[0]\n",
    "# print(n_sources)\n",
    "# bounds = torch.tensor([[-5, 0, 0], [10, 15, n_sources - 1]], **tkwargs)\n",
    "# target_fidelities = {n_sources - 1: 1.0}\n",
    "\n",
    "# cost_model = AffineFidelityCostModel(fidelity_weights=target_fidelities, fixed_cost=5.0)\n",
    "# cost_aware_utility = InverseCostWeightedUtility(cost_model=cost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e30344694a9583",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model initialization\n",
    "\n",
    "We use a `SingleTaskAugmentedGP` to implement our AGP.\n",
    "\n",
    "At each Bayesian Optimization iteration, the set of observations from the *ground-truth* (i.e., the highest fidelity and more expensive source) is temporarily *augmented* by including observations from the other cheap sources, only if they can be considered *reliable*. Specifically, an observation $(x,y)$ from a cheap source is considered reliable if it satisfies the following inequality:\n",
    "\n",
    "$$\\vert\\mu(x)-y\\vert \\leq m \\sigma(x)$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma(x)$ are, respectively, the posterior mean and standard deviation of the GP model fitted on the high fidelity observations only, and $m$ is a technical parameter making more *conservative* ($m→0$) or *inclusive* ($m→∞)$ the augmentation process. As reported in [1], a suitable value for this parameter is $m=1$.\n",
    "\n",
    "After the set of observations is augmented, the AGP is fitted through `SingleTaskAugmentedGP`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8272160f69227ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:07.801808200Z",
     "start_time": "2024-02-08T07:55:07.797392200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_initial_data(n):\n",
    "    train_x = get_random_x_for_agp(n, bounds, 1)\n",
    "    xs = train_x[..., :-1]\n",
    "    fids = fidelities[train_x[..., -1].int()].reshape(-1, 1)\n",
    "    train_obj = problem(torch.cat((xs, fids), dim=1)).unsqueeze(-1)\n",
    "    return train_x, train_obj\n",
    "\n",
    "\n",
    "def initialize_model(train_x, train_obj, m):\n",
    "    model = SingleTaskAugmentedGP(\n",
    "        train_x, train_obj, m=m, outcome_transform=Standardize(m=1),\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21cd7999805d78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "This helper function optimizes the acquisition function and returns the candidate point along with the observed function values.\n",
    "\n",
    "The UCB acquisition function has been modified to deal with both the *discrepancy* between information sources and the *source-specific query cost*.\n",
    "\n",
    "Formally, the AUCB acquisition function, at a generic iteration $t$, is defined as:\n",
    "\n",
    "$$\\alpha_s(x,\\hat y^+) = \\frac{\\left[\\hat{\\mu}(x) + \\sqrt{\\beta^{(t)}} \\hat{\\sigma}(x)\\right] - \\hat{y}^+}{c_s \\cdot (1+\\vert \\hat{\\mu}(x) - \\mu_s(x) \\vert)} $$\n",
    "\n",
    "where $\\hat{y}^+$ is the best (i.e., highest) value in the *augmented* set of observations, the numerator is -- therefore -- the optimistic improvement with respect to $\\hat{y}^+$, $c_s$ is the query cost for the source $s$, and $\\vert \\hat{\\mu}(x) - \\mu_s(x) \\vert$ is a discrepancy measure between the predictions provided by the AGP and the GP on the source $s$, respectively, given the input $x$ (i.e., 1 is added just to avoid division by zero).\n",
    "\n",
    "For more information, please refer to [1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "311309bd6a4d3a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:07.811037100Z",
     "start_time": "2024-02-08T07:55:07.802897300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_aucb(acqf):\n",
    "    candidate, value = optimize_acqf(\n",
    "        acq_function=acqf,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=128,\n",
    "    )\n",
    "    # observe new values\n",
    "    new_x = candidate.detach()\n",
    "    new_x[:, -1] = torch.round(new_x[:, -1], decimals=0)\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b55ca7ae58af3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Perform a few steps of multi-fidelity BO\n",
    "First, let's generate some initial random data and fit a surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2b9dbfbae2f7d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:55:10.200578200Z",
     "start_time": "2024-02-08T07:55:07.808815500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "train_x, train_obj = generate_initial_data(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93f06d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.5318e+02, 2.4700e+03, 7.6917e+03, 6.8851e+02, 4.1297e+03, 3.3466e+02,\n",
       "          1.6212e+03, 3.7243e+03, 2.9720e+03, 2.9985e+03, 1.0000e+00],\n",
       "         [7.5227e+03, 1.0500e+04, 4.7819e+03, 1.0585e+04, 8.1834e+03, 9.8811e+03,\n",
       "          8.3993e+03, 7.5654e+03, 7.8570e+03, 8.9943e+03, 1.0000e+00],\n",
       "         [1.0915e+04, 5.1708e+03, 8.3359e+03, 5.3681e+03, 1.0536e+04, 6.1054e+03,\n",
       "          5.4461e+03, 1.0128e+04, 1.0755e+04, 7.2204e+03, 0.0000e+00],\n",
       "         [2.7732e+03, 7.7972e+03, 1.2140e+03, 6.6498e+03, 1.8598e+03, 4.6482e+03,\n",
       "          6.5511e+03, 5.6050e+02, 4.1736e+02, 2.6023e+03, 2.0000e+00],\n",
       "         [5.1908e+03, 3.5431e+03, 3.6410e+03, 7.4364e+03, 9.7679e+02, 2.6772e+03,\n",
       "          3.3168e+03, 9.4177e+03, 9.5916e+03, 1.0627e+04, 0.0000e+00]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.9041e+17],\n",
       "         [-7.3237e+20],\n",
       "         [-9.8641e+21],\n",
       "         [-8.7202e+17],\n",
       "         [-9.6257e+19]], dtype=torch.float64))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54230c1481ba60",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now use the helper functions above to run a few iterations of BO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515beba",
   "metadata": {},
   "source": [
    "In questo problema ci sono 3 fonti,  0 ha costo 5.5, 1 ha costo 7.75 e 2 costo 6.00\n",
    "    {i: fid + 5.0 for i, fid in enumerate(fidelities)} -> {0: tensor(5.5000, dtype...h.float64), 1: tensor(5.7500, dtype...h.float64), 2: tensor(6., dtype=tor...h.float64)}\n",
    "L'ultimo valore di ogni x rappresenta la fonte id Fid \n",
    "    new_x[:, -1][0] -> 1.00  |>>| ter 0;\t Fid = 1.00;\t Obj = -200.3252;\n",
    "\n",
    "dunque train_x ha (in questo caso) le prime due dimensioni che rappresentano le informazioni del punto e la terza che rappresenta la source\n",
    "    tensor([[-3.9730,  3.3679,  1.0000],\n",
    "            [ 3.2453, 10.8819,  1.0000],\n",
    "            [ 8.0466,  7.0303,  2.0000],\n",
    "            [-1.0395, 14.4851,  0.0000],\n",
    "            [ 1.8883,  4.8380,  1.0000],\n",
    "            [ 5.0308, 14.9492,  2.0000],\n",
    "            [ 8.0466,  7.0303,  2.0000],\n",
    "            [ 8.0466,  7.0302,  2.0000]], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d02e319798a28d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T07:59:16.933226800Z",
     "start_time": "2024-02-08T07:55:10.205902900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0;\t Fid = 1.00;\t Obj = -25848880479644758016.0000;\n",
      "Iter 1;\t Fid = 1.00;\t Obj = -400313830635190336.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2;\t Fid = 1.00;\t Obj = -702897826604901597184.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3;\t Fid = 1.00;\t Obj = -5789263563085260800.0000;\n",
      "Iter 4;\t Fid = 1.00;\t Obj = -194146102532375707648.0000;\n",
      "Iter 5;\t Fid = 1.00;\t Obj = -1048828111299228729344.0000;\n",
      "Iter 6;\t Fid = 1.00;\t Obj = -50779831892422492160.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7;\t Fid = 0.75;\t Obj = -962232432092239233024.0000;\n",
      "Iter 8;\t Fid = 0.75;\t Obj = -503588045707247878144.0000;\n",
      "Iter 9;\t Fid = 0.75;\t Obj = -1032563552034850865152.0000;\n",
      "Iter 10;\t Fid = 1.00;\t Obj = -18201535289894926336.0000;\n",
      "Iter 11;\t Fid = 1.00;\t Obj = -1026065795867141603328.0000;\n",
      "Iter 12;\t Fid = 0.75;\t Obj = -577362955074147975168.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n",
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13;\t Fid = 1.00;\t Obj = -26785928349472698368.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n",
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14;\t Fid = 1.00;\t Obj = -1497650117723517091840.0000;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:102: OptimizationWarning: `scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m botorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mvalidate_input_scaling(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_ITER):\n\u001b[0;32m----> 5\u001b[0m         mll, model \u001b[38;5;241m=\u001b[39m initialize_model(train_x, train_obj, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m         fit_gpytorch_mll(mll)\n\u001b[1;32m      7\u001b[0m         acqf \u001b[38;5;241m=\u001b[39m AugmentedUpperConfidenceBound(\n\u001b[1;32m      8\u001b[0m             model,\n\u001b[1;32m      9\u001b[0m             beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m             cost\u001b[38;5;241m=\u001b[39m{i: fid \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, fid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fidelities)},\n\u001b[1;32m     13\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[59], line 10\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(train_x, train_obj, m)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_model\u001b[39m(train_x, train_obj, m):\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m SingleTaskAugmentedGP(\n\u001b[1;32m     11\u001b[0m         train_x, train_obj, m\u001b[38;5;241m=\u001b[39mm, outcome_transform\u001b[38;5;241m=\u001b[39mStandardize(m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     mll \u001b[38;5;241m=\u001b[39m ExactMarginalLogLikelihood(model\u001b[38;5;241m.\u001b[39mlikelihood, model)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mll, model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch_community/models/gp_regression_multisource.py:139\u001b[0m, in \u001b[0;36mSingleTaskAugmentedGP.__init__\u001b[0;34m(self, train_X, train_Y, train_Yvar, m, likelihood, covar_module, mean_module, outcome_transform, input_transform)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_n_cheap_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(points) \u001b[38;5;28;01mfor\u001b[39;00m points \u001b[38;5;129;01min\u001b[39;00m train_X[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Init and fit a SingleTaskGP for each source\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fit_gp(\n\u001b[1;32m    141\u001b[0m         train_X[s][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    142\u001b[0m         train_Y[s],\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train_Yvar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m train_Yvar[s],\n\u001b[1;32m    144\u001b[0m         likelihood,\n\u001b[1;32m    145\u001b[0m         covar_module,\n\u001b[1;32m    146\u001b[0m         mean_module,\n\u001b[1;32m    147\u001b[0m         outcome_transform,\n\u001b[1;32m    148\u001b[0m         input_transform,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sources\n\u001b[1;32m    151\u001b[0m ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Create the training set for the AGP selecting all\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# the observations from the high fidelity source\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# and the reliable observations from the other sources\u001b[39;00m\n\u001b[1;32m    156\u001b[0m reliable_idxs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    157\u001b[0m     _get_reliable_observations(\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[s], train_X[s][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sources[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    161\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch_community/models/gp_regression_multisource.py:140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_n_cheap_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(points) \u001b[38;5;28;01mfor\u001b[39;00m points \u001b[38;5;129;01min\u001b[39;00m train_X[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Init and fit a SingleTaskGP for each source\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fit_gp(\n\u001b[1;32m    141\u001b[0m         train_X[s][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    142\u001b[0m         train_Y[s],\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train_Yvar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m train_Yvar[s],\n\u001b[1;32m    144\u001b[0m         likelihood,\n\u001b[1;32m    145\u001b[0m         covar_module,\n\u001b[1;32m    146\u001b[0m         mean_module,\n\u001b[1;32m    147\u001b[0m         outcome_transform,\n\u001b[1;32m    148\u001b[0m         input_transform,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sources\n\u001b[1;32m    151\u001b[0m ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Create the training set for the AGP selecting all\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# the observations from the high fidelity source\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# and the reliable observations from the other sources\u001b[39;00m\n\u001b[1;32m    156\u001b[0m reliable_idxs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    157\u001b[0m     _get_reliable_observations(\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[s], train_X[s][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sources[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    161\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch_community/models/gp_regression_multisource.py:239\u001b[0m, in \u001b[0;36mSingleTaskAugmentedGP._init_fit_gp\u001b[0;34m(self, train_X, train_Y, train_Yvar, likelihood, covar_module, mean_module, outcome_transform, input_transform)\u001b[0m\n\u001b[1;32m    228\u001b[0m gp \u001b[38;5;241m=\u001b[39m SingleTaskGP(\n\u001b[1;32m    229\u001b[0m     train_X,\n\u001b[1;32m    230\u001b[0m     train_Y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m     input_transform\u001b[38;5;241m=\u001b[39minput_transform,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m mll \u001b[38;5;241m=\u001b[39m ExactMarginalLogLikelihood(gp\u001b[38;5;241m.\u001b[39mlikelihood, gp)\n\u001b[0;32m--> 239\u001b[0m fit_gpytorch_mll(mll)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/fit.py:105\u001b[0m, in \u001b[0;36mfit_gpytorch_mll\u001b[0;34m(mll, closure, optimizer, closure_kwargs, optimizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# defer to per-method defaults\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FitGPyTorchMLL(\n\u001b[1;32m    106\u001b[0m     mll,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mtype\u001b[39m(mll\u001b[38;5;241m.\u001b[39mlikelihood),\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mtype\u001b[39m(mll\u001b[38;5;241m.\u001b[39mmodel),\n\u001b[1;32m    109\u001b[0m     closure\u001b[38;5;241m=\u001b[39mclosure,\n\u001b[1;32m    110\u001b[0m     closure_kwargs\u001b[38;5;241m=\u001b[39mclosure_kwargs,\n\u001b[1;32m    111\u001b[0m     optimizer_kwargs\u001b[38;5;241m=\u001b[39moptimizer_kwargs,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/utils/dispatcher.py:93\u001b[0m, in \u001b[0;36mDispatcher.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(types\u001b[38;5;241m=\u001b[39mtypes)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MDNotImplementedError:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Traverses registered methods in order, yields whenever a match is found\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_iter(\u001b[38;5;241m*\u001b[39mtypes)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/fit.py:252\u001b[0m, in \u001b[0;36m_fit_fallback\u001b[0;34m(mll, _, __, closure, optimizer, closure_kwargs, optimizer_kwargs, max_attempts, warning_handler, caught_exception_types, **ignore)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m warning_list, debug(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    251\u001b[0m     simplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mOptimizationWarning)\n\u001b[0;32m--> 252\u001b[0m     optimizer(mll, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_kwargs)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Resolved warnings and determine whether or not to retry\u001b[39;00m\n\u001b[1;32m    255\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/fit.py:92\u001b[0m, in \u001b[0;36mfit_gpytorch_mll_scipy\u001b[0;34m(mll, parameters, bounds, closure, closure_kwargs, method, options, callback, timeout_sec)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclosure_kwargs)\n\u001b[0;32m---> 92\u001b[0m result \u001b[38;5;241m=\u001b[39m scipy_minimize(\n\u001b[1;32m     93\u001b[0m     closure\u001b[38;5;241m=\u001b[39mclosure,\n\u001b[1;32m     94\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m     95\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m     96\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m     97\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m     98\u001b[0m     callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m     99\u001b[0m     timeout_sec\u001b[38;5;241m=\u001b[39mtimeout_sec,\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m OptimizationStatus\u001b[38;5;241m.\u001b[39mSUCCESS:\n\u001b[1;32m    102\u001b[0m     warn(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`scipy_minimize` terminated with status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, displaying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m original message from `scipy.optimize.minimize`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m         OptimizationWarning,\n\u001b[1;32m    106\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/core.py:109\u001b[0m, in \u001b[0;36mscipy_minimize\u001b[0;34m(closure, parameters, bounds, callback, x0, method, options, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m         result \u001b[38;5;241m=\u001b[39m OptimizationResult(\n\u001b[1;32m    102\u001b[0m             step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(call_counter),\n\u001b[1;32m    103\u001b[0m             fval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(wrapped_closure(x)[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    104\u001b[0m             status\u001b[38;5;241m=\u001b[39mOptimizationStatus\u001b[38;5;241m.\u001b[39mRUNNING,\n\u001b[1;32m    105\u001b[0m             runtime\u001b[38;5;241m=\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time,\n\u001b[1;32m    106\u001b[0m         )\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callback(parameters, result)  \u001b[38;5;66;03m# pyre-ignore [29]\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m raw \u001b[38;5;241m=\u001b[39m minimize_with_timeout(\n\u001b[1;32m    110\u001b[0m     wrapped_closure,\n\u001b[1;32m    111\u001b[0m     wrapped_closure\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mif\u001b[39;00m x0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x0\u001b[38;5;241m.\u001b[39mastype(np_float64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    112\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds_np,\n\u001b[1;32m    114\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    115\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    116\u001b[0m     callback\u001b[38;5;241m=\u001b[39mwrapped_callback,\n\u001b[1;32m    117\u001b[0m     timeout_sec\u001b[38;5;241m=\u001b[39mtimeout_sec,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Post-processing and outcome handling\u001b[39;00m\n\u001b[1;32m    121\u001b[0m wrapped_closure\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m asarray(raw\u001b[38;5;241m.\u001b[39mx)  \u001b[38;5;66;03m# set parameter state to optimal values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/utils/timeout.py:80\u001b[0m, in \u001b[0;36mminimize_with_timeout\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options, timeout_sec)\u001b[0m\n\u001b[1;32m     77\u001b[0m     wrapped_callback \u001b[38;5;241m=\u001b[39m callback\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[1;32m     81\u001b[0m         fun\u001b[38;5;241m=\u001b[39mfun,\n\u001b[1;32m     82\u001b[0m         x0\u001b[38;5;241m=\u001b[39mx0,\n\u001b[1;32m     83\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     84\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m     85\u001b[0m         jac\u001b[38;5;241m=\u001b[39mjac,\n\u001b[1;32m     86\u001b[0m         hess\u001b[38;5;241m=\u001b[39mhess,\n\u001b[1;32m     87\u001b[0m         hessp\u001b[38;5;241m=\u001b[39mhessp,\n\u001b[1;32m     88\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m     89\u001b[0m         constraints\u001b[38;5;241m=\u001b[39mconstraints,\n\u001b[1;32m     90\u001b[0m         tol\u001b[38;5;241m=\u001b[39mtol,\n\u001b[1;32m     91\u001b[0m         callback\u001b[38;5;241m=\u001b[39mwrapped_callback,\n\u001b[1;32m     92\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OptimizationTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    711\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:361\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    355\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/closures/core.py:150\u001b[0m, in \u001b[0;36mNdarrayOptimizationClosure.__call__\u001b[0;34m(self, state, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     value_tensor, grad_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_array(value_tensor)\n\u001b[1;32m    152\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_ndarray(fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/closures/core.py:64\u001b[0m, in \u001b[0;36mForwardBackwardClosure.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_manager():\n\u001b[0;32m---> 64\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     65\u001b[0m         value \u001b[38;5;241m=\u001b[39m values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreducer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreducer(values)\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botorch/optim/closures/model_closures.py:176\u001b[0m, in \u001b[0;36m_get_loss_closure_exact_internal.<locals>.closure\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    175\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m mll\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mmll\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_inputs)\n\u001b[0;32m--> 176\u001b[0m     log_likelihood \u001b[38;5;241m=\u001b[39m mll(\n\u001b[1;32m    177\u001b[0m         model_output, mll\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_targets, \u001b[38;5;241m*\u001b[39mmll\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlog_likelihood\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:65\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     64\u001b[0m res \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlog_prob(target)\n\u001b[0;32m---> 65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n\u001b[1;32m     68\u001b[0m num_data \u001b[38;5;241m=\u001b[39m function_dist\u001b[38;5;241m.\u001b[39mevent_shape\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:38\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood._add_other_terms\u001b[0;34m(self, res, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_other_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, res, params):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m added_loss_term \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39madded_loss_terms():\n\u001b[1;32m     39\u001b[0m         res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39madd(added_loss_term\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;241m*\u001b[39mparams))\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Add log probs of priors on the (functions of) parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:57\u001b[0m, in \u001b[0;36mModule.added_loss_terms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madded_loss_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, strategy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_added_loss_terms():\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m strategy\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:514\u001b[0m, in \u001b[0;36m_extract_named_added_loss_terms\u001b[0;34m(module, memo, prefix)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mname, module_ \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    513\u001b[0m     submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m mname\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, strategy \u001b[38;5;129;01min\u001b[39;00m _extract_named_added_loss_terms(module\u001b[38;5;241m=\u001b[39mmodule_, memo\u001b[38;5;241m=\u001b[39mmemo, prefix\u001b[38;5;241m=\u001b[39msubmodule_prefix):\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m name, strategy\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:514\u001b[0m, in \u001b[0;36m_extract_named_added_loss_terms\u001b[0;34m(module, memo, prefix)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mname, module_ \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    513\u001b[0m     submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m mname\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, strategy \u001b[38;5;129;01min\u001b[39;00m _extract_named_added_loss_terms(module\u001b[38;5;241m=\u001b[39mmodule_, memo\u001b[38;5;241m=\u001b[39mmemo, prefix\u001b[38;5;241m=\u001b[39msubmodule_prefix):\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m name, strategy\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:514\u001b[0m, in \u001b[0;36m_extract_named_added_loss_terms\u001b[0;34m(module, memo, prefix)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mname, module_ \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    513\u001b[0m     submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m mname\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, strategy \u001b[38;5;129;01min\u001b[39;00m _extract_named_added_loss_terms(module\u001b[38;5;241m=\u001b[39mmodule_, memo\u001b[38;5;241m=\u001b[39mmemo, prefix\u001b[38;5;241m=\u001b[39msubmodule_prefix):\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m name, strategy\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gpytorch/module.py:507\u001b[0m, in \u001b[0;36m_extract_named_added_loss_terms\u001b[0;34m(module, memo, prefix)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_added_loss_terms\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, strategy \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_added_loss_terms\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m strategy \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cumulative_cost = 0.0\n",
    "\n",
    "with botorch.settings.validate_input_scaling(False):\n",
    "    for it in range(N_ITER):\n",
    "        mll, model = initialize_model(train_x, train_obj, m=1)\n",
    "        fit_gpytorch_mll(mll)\n",
    "        acqf = AugmentedUpperConfidenceBound(\n",
    "            model,\n",
    "            beta=3,\n",
    "            maximize=True,\n",
    "            best_f=train_obj[torch.where(train_x[:, -1] == 0)].min(),\n",
    "            cost={i: fid + 5.0 for i, fid in enumerate(fidelities)},\n",
    "        )\n",
    "        new_x = optimize_aucb(acqf)\n",
    "        if model.n_true_points < model.max_n_cheap_points:\n",
    "            new_x[:, -1] = fidelities.shape[0] - 1\n",
    "        train_x = torch.cat([train_x, new_x])\n",
    "\n",
    "        new_x[:, -1] = fidelities[new_x[:, -1].int()]\n",
    "        new_obj = problem(new_x).unsqueeze(-1)\n",
    "        train_obj = torch.cat([train_obj, new_obj])\n",
    "\n",
    "        print(\n",
    "            f\"Iter {it};\"\n",
    "            f\"\\t Fid = {new_x[0].tolist()[-1]:.2f};\"\n",
    "            f\"\\t Obj = {new_obj[0][0].tolist():.4f};\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
